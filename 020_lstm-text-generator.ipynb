{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import the project Gutenberg dataset\nfrom nltk.corpus import gutenberg","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:26:01.187449Z","iopub.execute_input":"2023-04-24T10:26:01.188184Z","iopub.status.idle":"2023-04-24T10:26:02.516519Z","shell.execute_reply.started":"2023-04-24T10:26:01.188154Z","shell.execute_reply":"2023-04-24T10:26:02.515225Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"gutenberg.fileids()","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:26:02.523789Z","iopub.execute_input":"2023-04-24T10:26:02.526531Z","iopub.status.idle":"2023-04-24T10:26:02.544304Z","shell.execute_reply.started":"2023-04-24T10:26:02.526484Z","shell.execute_reply":"2023-04-24T10:26:02.543260Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['austen-emma.txt',\n 'austen-persuasion.txt',\n 'austen-sense.txt',\n 'bible-kjv.txt',\n 'blake-poems.txt',\n 'bryant-stories.txt',\n 'burgess-busterbrown.txt',\n 'carroll-alice.txt',\n 'chesterton-ball.txt',\n 'chesterton-brown.txt',\n 'chesterton-thursday.txt',\n 'edgeworth-parents.txt',\n 'melville-moby_dick.txt',\n 'milton-paradise.txt',\n 'shakespeare-caesar.txt',\n 'shakespeare-hamlet.txt',\n 'shakespeare-macbeth.txt',\n 'whitman-leaves.txt']"},"metadata":{}}]},{"cell_type":"code","source":"# preprocess Shakespeare plays\ntext = ''\nfor txt in gutenberg.fileids():\n    if 'shakespeare' in txt:\n        text += gutenberg.raw(txt).lower()\nchars = sorted(list(set(text)))\n#make a dict of character to an index, for reference in the one-hot encoding\nchar_indices = dict((c, i) for i, c in enumerate(chars))\n#make the opposite dict for lookup when interpreting the one-hot encoding back to the char\nindices_char = dict((i, c) for i, c in enumerate(chars))\n\nf\"corpus length: {len(text)} total chars: {len(chars)}\"","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:26:02.548662Z","iopub.execute_input":"2023-04-24T10:26:02.551076Z","iopub.status.idle":"2023-04-24T10:26:02.577116Z","shell.execute_reply.started":"2023-04-24T10:26:02.551036Z","shell.execute_reply":"2023-04-24T10:26:02.576087Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'corpus length: 375542 total chars: 50'"},"metadata":{}}]},{"cell_type":"code","source":"print(text[:500])","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:26:02.582960Z","iopub.execute_input":"2023-04-24T10:26:02.585335Z","iopub.status.idle":"2023-04-24T10:26:02.595456Z","shell.execute_reply.started":"2023-04-24T10:26:02.585296Z","shell.execute_reply":"2023-04-24T10:26:02.594231Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[the tragedie of julius caesar by william shakespeare 1599]\n\n\nactus primus. scoena prima.\n\nenter flauius, murellus, and certaine commoners ouer the stage.\n\n  flauius. hence: home you idle creatures, get you home:\nis this a holiday? what, know you not\n(being mechanicall) you ought not walke\nvpon a labouring day, without the signe\nof your profession? speake, what trade art thou?\n  car. why sir, a carpenter\n\n   mur. where is thy leather apron, and thy rule?\nwhat dost thou with thy best apparrell on\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next weâ€™re going to chop up the source text into data samples, each with a fixed, maxlen set of characters.","metadata":{}},{"cell_type":"code","source":"list(range(0, 50, 3))","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:26:02.600510Z","iopub.execute_input":"2023-04-24T10:26:02.602979Z","iopub.status.idle":"2023-04-24T10:26:02.614929Z","shell.execute_reply.started":"2023-04-24T10:26:02.602938Z","shell.execute_reply":"2023-04-24T10:26:02.613628Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48]"},"metadata":{}}]},{"cell_type":"code","source":"maxlen = 40\nstep = 3\nsentences = []\nnext_chars = []\n\n#step by 3 characters, so the generated training sample will overlap, but not be identical\nfor i in range(0, len(text) - maxlen, step):\n    #grab a slice of text\n    sentences.append(text[i: i + maxlen])\n    #collect the next expected char\n    next_chars.append(text[i + maxlen])\nprint('nb sequences: ', len(sentences))","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:26:02.618862Z","iopub.execute_input":"2023-04-24T10:26:02.619632Z","iopub.status.idle":"2023-04-24T10:26:02.749324Z","shell.execute_reply.started":"2023-04-24T10:26:02.619593Z","shell.execute_reply":"2023-04-24T10:26:02.748209Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"nb sequences:  125168\n","output_type":"stream"}]},{"cell_type":"markdown","source":"So we have 125,168 training samples and the character that follows each of them, the target for our model.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n#one-hot encode the training examples\nX = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\ny = np.zeros((len(sentences), len(chars)), dtype=bool)\n\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        X[i, t, char_indices[char]] = 1\n    y[i, char_indices[next_chars[i]]] = 1","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:26:02.753730Z","iopub.execute_input":"2023-04-24T10:26:02.755166Z","iopub.status.idle":"2023-04-24T10:26:05.723192Z","shell.execute_reply.started":"2023-04-24T10:26:02.755127Z","shell.execute_reply":"2023-04-24T10:26:05.721580Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#assemble a character-based LSTM model for generating text\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\n\nmodel = Sequential()\n\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation('softmax'))\noptimizer = RMSprop(learning_rate=0.01)\n\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:26:05.724746Z","iopub.execute_input":"2023-04-24T10:26:05.725131Z","iopub.status.idle":"2023-04-24T10:26:16.394446Z","shell.execute_reply.started":"2023-04-24T10:26:05.725087Z","shell.execute_reply":"2023-04-24T10:26:16.393526Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 128)               91648     \n                                                                 \n dense (Dense)               (None, 50)                6450      \n                                                                 \n activation (Activation)     (None, 50)                0         \n                                                                 \n=================================================================\nTotal params: 98,098\nTrainable params: 98,098\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"#train shakespearean chatbot\nepochs = 6\nbatch_size = 128\nmodel_structure = model.to_json()\n\nwith open('shakes_lstm_model.json', \"w\") as json_file:\n    json_file.write(model_structure)\n    \nfor i in range(5):\n    model.fit(X, y, batch_size=batch_size, epochs=epochs)\n    model.save_weights(f\"shakes_lstm_weights_{i+1}.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:26:16.395899Z","iopub.execute_input":"2023-04-24T10:26:16.396284Z","iopub.status.idle":"2023-04-24T10:29:36.242870Z","shell.execute_reply.started":"2023-04-24T10:26:16.396248Z","shell.execute_reply":"2023-04-24T10:29:36.241829Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/6\n978/978 [==============================] - 11s 6ms/step - loss: 2.0698\nEpoch 2/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.6960\nEpoch 3/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.5862\nEpoch 4/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.5212\nEpoch 5/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.4751\nEpoch 6/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.4440\nEpoch 1/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.4190\nEpoch 2/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.3958\nEpoch 3/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.3796\nEpoch 4/6\n978/978 [==============================] - 5s 5ms/step - loss: 1.3632\nEpoch 5/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.3530\nEpoch 6/6\n978/978 [==============================] - 5s 5ms/step - loss: 1.3389\nEpoch 1/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.3293\nEpoch 2/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.3205\nEpoch 3/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.3132\nEpoch 4/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.3048\nEpoch 5/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.2992\nEpoch 6/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.2930\nEpoch 1/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.2835\nEpoch 2/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.2805\nEpoch 3/6\n978/978 [==============================] - 5s 5ms/step - loss: 1.2749\nEpoch 4/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.2681\nEpoch 5/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.2637\nEpoch 6/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.2596\nEpoch 1/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.2553\nEpoch 2/6\n978/978 [==============================] - 7s 7ms/step - loss: 1.2484\nEpoch 3/6\n978/978 [==============================] - 7s 7ms/step - loss: 1.2428\nEpoch 4/6\n978/978 [==============================] - 6s 6ms/step - loss: 1.2426\nEpoch 5/6\n978/978 [==============================] - 5s 5ms/step - loss: 1.2378\nEpoch 6/6\n978/978 [==============================] - 5s 6ms/step - loss: 1.2332\n","output_type":"stream"}]},{"cell_type":"code","source":"#sampler to generate character sequences\nimport random\n\ndef sample(preds, temperature=1.0):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds)/temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds/np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:29:36.247194Z","iopub.execute_input":"2023-04-24T10:29:36.247492Z","iopub.status.idle":"2023-04-24T10:29:36.253887Z","shell.execute_reply.started":"2023-04-24T10:29:36.247464Z","shell.execute_reply":"2023-04-24T10:29:36.252727Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#generate three texts with three diversity levels\nimport sys\n\nstart_index = random.randint(0, len(text) - maxlen - 1)\nfor diversity in [0.2, 0.5, 1.0]:\n    print()\n    print('------ diversity:', diversity)\n    generated = ''\n    sentence = text[start_index: start_index + maxlen]\n    generated += sentence\n    print('----- Generating with seed: \"' + sentence + '\"')\n    sys.stdout.write(generated)\n    \n    for i in range(400):\n        x = np.zeros((1, maxlen, len(chars)))\n        for t, char in enumerate(sentence):\n            x[0, t, char_indices[char]] = 1\n        preds = model.predict(x, verbose=0)[0]\n        next_index = sample(preds, diversity)\n        next_char = indices_char[next_index]\n        generated += next_char\n        sentence = sentence[1:] + next_char\n        sys.stdout.write(next_char)\n        #sys.stdout.write(next_char)\n        sys.stdout.flush()\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-04-24T10:29:36.255452Z","iopub.execute_input":"2023-04-24T10:29:36.256071Z","iopub.status.idle":"2023-04-24T10:30:46.391293Z","shell.execute_reply.started":"2023-04-24T10:29:36.256032Z","shell.execute_reply":"2023-04-24T10:30:46.389840Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\n------ diversity: 0.2\n----- Generating with seed: \"h,\nby many of these traines, hath sought\"\nh,\nby many of these traines, hath soughting:\nthat i shall i haue seene of the purpose\nof my selfe to the marches and for they shake\nto the selfe of the winde of march of the state\nto the coursty of the pretter the selues:\nand then the loot a country of the selues\nall the convvsw-words: that we will for the strong of the selues\nall the seate of my stranne of the state of the death,\nand that i must be common him to the selues\nall the seat\n\n------ diversity: 0.5\n----- Generating with seed: \"h,\nby many of these traines, hath sought\"\nh,\nby many of these traines, hath sought,\nthat we to do? did you are need, and marke,\nwhich i my lord, stand strong into much his march.\nor your sweet marry then to make him to the prayers hand,\nthat i may thou shalt be done, and march it forreparted;\nit shall follow them condent friends so behildonight\nhath made him ear't them done to the selugue,\nwell senstlesse of his pasticiues of propos:\nand then thou art i so from the dooke,\nand t\n\n------ diversity: 1.0\n----- Generating with seed: \"h,\nby many of these traines, hath sought\"\nh,\nby many of these traines, hath soughtains with vs.\nthou, where is he wilts in the streets;\nand then thou to his man at strawly country: beele\nwith usident vertue it damned know\novomg'd: me thou\nfinde kinde be king more:\nhardon out to our tongue: or this strong reueres to the doomede,\nto say alone downe\n\n   ant. reuoly inpr after finde been very march was\nwith all it comely old: a fiome is denmarke;\nbuent from the possuers was this iu\n","output_type":"stream"}]}]}