{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0M47FElDD1vWmygdoPWyN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E8rpJHJzux64","executionInfo":{"status":"ok","timestamp":1682666470419,"user_tz":-300,"elapsed":11462,"user":{"displayName":"Bizoo Bizoo","userId":"08505249684389076845"}},"outputId":"d13fca3b-1b7a-4455-cf1b-b826915eadae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","source":["## Loading Tokenizer\n","Similar to TFAutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint"],"metadata":{"id":"dTrIMNl7zzgz"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"],"metadata":{"id":"xYVQ-j9jvDhy","executionInfo":{"status":"ok","timestamp":1682667773131,"user_tz":-300,"elapsed":2,"user":{"displayName":"Bizoo Bizoo","userId":"08505249684389076845"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Encoding"],"metadata":{"id":"096GY69k0K3O"}},{"cell_type":"markdown","source":["### Tokenization"],"metadata":{"id":"CoaC_9NS0Sjx"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","sequence = \"I am learning Hugging Face by following their course.\"\n","tokens = tokenizer.tokenize(sequence)\n","\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tzbz8NUL0Grh","executionInfo":{"status":"ok","timestamp":1682667930376,"user_tz":-300,"elapsed":822,"user":{"displayName":"Bizoo Bizoo","userId":"08505249684389076845"}},"outputId":"6a8b023d-694d-424b-f8d0-9afdaf02c4f4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'am', 'learning', 'Hu', '##gging', 'Face', 'by', 'following', 'their', 'course', '.']\n"]}]},{"cell_type":"markdown","source":["This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. That’s the case here with hugging, which is split into two tokens: Hu and ##gging."],"metadata":{"id":"q-Vrae4I04EV"}},{"cell_type":"markdown","source":["### From tokens to input ids\n","The conversion to input IDs is handled by the convert_tokens_to_ids() tokenizer method"],"metadata":{"id":"pidGw-ie1BFH"}},{"cell_type":"code","source":["ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNuu2jo00vhs","executionInfo":{"status":"ok","timestamp":1682668068296,"user_tz":-300,"elapsed":6,"user":{"displayName":"Bizoo Bizoo","userId":"08505249684389076845"}},"outputId":"bb47cb70-2dbd-4377-8335-df638462692f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[146, 1821, 3776, 20164, 10932, 10289, 1118, 1378, 1147, 1736, 119]\n"]}]},{"cell_type":"markdown","source":["## Decoding"],"metadata":{"id":"U5AilfDZ1VU8"}},{"cell_type":"code","source":["decoded_string = tokenizer.decode(ids)\n","print(decoded_string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sLsGmUq81Ray","executionInfo":{"status":"ok","timestamp":1682668133191,"user_tz":-300,"elapsed":8833,"user":{"displayName":"Bizoo Bizoo","userId":"08505249684389076845"}},"outputId":"014f60e4-ca1b-4e6d-eeae-6cc4f376ea3a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["I am learning Hugging Face by following their course.\n"]}]}]}